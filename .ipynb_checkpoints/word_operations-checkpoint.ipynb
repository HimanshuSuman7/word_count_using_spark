{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Moby-Dick.txt MapPartitionsRDD[1] at textFile at <unknown>:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = sc.textFile(\"Moby-Dick.txt\")\n",
    "text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[3] at RDD at PythonRDD.scala:53 []',\n",
       " ' |  Moby-Dick.txt MapPartitionsRDD[1] at textFile at <unknown>:0 []',\n",
       " ' |  Moby-Dick.txt HadoopRDD[0] at textFile at <unknown>:0 []']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = text_file.flatMap(lambda line: line.split(\" \"))\n",
    "word_list.toDebugString().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[4] at RDD at PythonRDD.scala:53 []',\n",
       " ' |  Moby-Dick.txt MapPartitionsRDD[1] at textFile at <unknown>:0 []',\n",
       " ' |  Moby-Dick.txt HadoopRDD[0] at textFile at <unknown>:0 []']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_empty = word_list.filter(lambda ne: ne != \"\")\n",
    "not_empty.toDebugString().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[5] at RDD at PythonRDD.scala:53 []',\n",
       " ' |  Moby-Dick.txt MapPartitionsRDD[1] at textFile at <unknown>:0 []',\n",
       " ' |  Moby-Dick.txt HadoopRDD[0] at textFile at <unknown>:0 []']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_values = not_empty.map(lambda word: (word, 1))\n",
    "key_values.toDebugString().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[10] at RDD at PythonRDD.scala:53 []',\n",
       " ' |  MapPartitionsRDD[9] at mapPartitions at PythonRDD.scala:133 []',\n",
       " ' |  ShuffledRDD[8] at partitionBy at <unknown>:0 []',\n",
       " ' +-(2) PairwiseRDD[7] at reduceByKey at <ipython-input-10-ca778e8096d1>:1 []',\n",
       " '    |  PythonRDD[6] at reduceByKey at <ipython-input-10-ca778e8096d1>:1 []',\n",
       " '    |  Moby-Dick.txt MapPartitionsRDD[1] at textFile at <unknown>:0 []',\n",
       " '    |  Moby-Dick.txt HadoopRDD[0] at textFile at <unknown>:0 []']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = key_values.reduceByKey(lambda a,b: a+b)\n",
    "counts.toDebugString().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words : 33781\n"
     ]
    }
   ],
   "source": [
    "total = counts.count()\n",
    "print(\"Different words : {}\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the file : 215133\n"
     ]
    }
   ],
   "source": [
    "words_sum = counts.map(lambda x: x[1]).reduce(lambda x,y: x+y)\n",
    "print(\"Total words in the file : {}\".format(words_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean occurence of words : 6.368461561232645\n"
     ]
    }
   ],
   "source": [
    "mean_occ = words_sum / total\n",
    "print(\"Mean occurence of words : {}\".format(mean_occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding most common words\n",
    "list_of_words = counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words \n",
      "the : 13766\n",
      "of : 6587\n",
      "and : 5951\n",
      "a : 4533\n",
      "to : 4510\n"
     ]
    }
   ],
   "source": [
    "# We are not using parallelism in this approach.\n",
    "list_of_words.sort(key=lambda x:x[1])\n",
    "print(\"Most common words \\n\" + \"\\n\".join([\"{} : {}\".format(x[0], x[1]) for x in reversed(list_of_words[-5:])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[13] at RDD at PythonRDD.scala:53 []',\n",
       " ' |  MapPartitionsRDD[9] at mapPartitions at PythonRDD.scala:133 []',\n",
       " ' |  ShuffledRDD[8] at partitionBy at <unknown>:0 []',\n",
       " ' +-(2) PairwiseRDD[7] at reduceByKey at <ipython-input-10-ca778e8096d1>:1 []',\n",
       " '    |  PythonRDD[6] at reduceByKey at <ipython-input-10-ca778e8096d1>:1 []',\n",
       " '    |  Moby-Dick.txt MapPartitionsRDD[1] at textFile at <unknown>:0 []',\n",
       " '    |  Moby-Dick.txt HadoopRDD[0] at textFile at <unknown>:0 []']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect top 5 elements using purely spark\n",
    "reverse_count = counts.map(lambda x: (x[1], x[0]))\n",
    "reverse_count.toDebugString().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) PythonRDD[20] at RDD at PythonRDD.scala:53 []',\n",
       " ' |  MapPartitionsRDD[19] at mapPartitions at PythonRDD.scala:133 []',\n",
       " ' |  ShuffledRDD[18] at partitionBy at <unknown>:0 []',\n",
       " ' +-(2) PairwiseRDD[17] at sortByKey at <ipython-input-22-add8bf707f50>:1 []',\n",
       " '    |  PythonRDD[16] at sortByKey at <ipython-input-22-add8bf707f50>:1 []',\n",
       " '    |  MapPartitionsRDD[9] at mapPartitions at PythonRDD.scala:133 []',\n",
       " '    |  ShuffledRDD[8] at partitionBy at <unknown>:0 []',\n",
       " '    +-(2) PairwiseRDD[7] at reduceByKey at <ipython-input-10-ca778e8096d1>:1 []',\n",
       " '       |  PythonRDD[6] at reduceByKey at <ipython-input-10-ca778e8096d1>:1 []',\n",
       " '       |  Moby-Dick.txt MapPartitionsRDD[1] at textFile at <unknown>:0 []',\n",
       " '       |  Moby-Dick.txt HadoopRDD[0] at textFile at <unknown>:0 []']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_keys = reverse_count.sortByKey(ascending=False)\n",
    "sort_keys.toDebugString().decode().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words \n",
      "13766 : the\n",
      "6587 : of\n",
      "5951 : and\n",
      "4533 : a\n",
      "4510 : to\n"
     ]
    }
   ],
   "source": [
    "result = sort_keys.take(5)\n",
    "print(\"Most common words \\n\" + \"\\n\".join([\"{} : {}\".format(x[0], x[1]) for x in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "1. Collecting and performing operations at the head node does not scale.\n",
    "2. Using RDDs to the end increases efficiency of the operations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
